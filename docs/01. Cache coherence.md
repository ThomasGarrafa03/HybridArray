# Cache coherence
### Cache lines
Nota la struttura "a piramide" delle memorie di un calcolatore, tra  **cache e RAM** esiste un concetto simile a quello tra **RAM e Disco**, secondo il quale la cache di un calcolatore è suddivisa in blocchi di dati della dimensione dei kb chiamati **cache line (linea di cache in italiano.)Queste cache line vengono caricate dalla memoria**, in modo tale che **ogni regione di memoria mappi su esattamente una cache line.**  

L'utilizzo delle cache line è dovuto al fatto che il processore si basa sulla cache per prelevare dati e istruzioni, in quanto **più veloce**. Il caricamento di una sequenza di dati e non di un singolo dato rende pertanto più efficiente il fetch di istruzioni e di dati e previene eventuali **bottleneck.**
  
Pertanto, se al processore servisse un singolo dato di questa cache line, esso sarà comunque costretto a dover caricare tutta la cache line.
### Cache coherence: che cos'è
La cache coherence è un meccanismo che consente di **mantenere i dati coerenti in memoria.** 
Nel caso di **architetture shared memory** sono richieste componenti hardware aggiuntive rispetto alle classiche reti di interconnessione (che forniscono meccanismi di base per il trasferimento di dati) per permettere **un accesso coordinato a dati che potrebbero avere più copie nella rete!**
#### Cache coherence: protocolli
-   **protocollo invalidante:**  la modifica invalida l'utilizzo di dati da parte di altri processori. eventuali ulteriori utilizzi da parte di questi ultimi della stessa cache line, causa un update. 
-   **protocollo di update:** ogni qualvolta un dato è scritto sulla cache line, tutte le altre eventuali copie vengono aggiornate.  
    (nota come, se un processore legge un dato una singola volta e non lo riutilizza più, eventuali update su questo dato (o sulla stessa copia della cache line in generale!) causa  **overhead eccessivo, non necessario!**)
Tra i due, il **protocollo invalidante** è il più usato: tutte le copie di quella cache line vengono invalidate, per poi successivamente essere modificate.  
#### Fenomeno del false sharing
Entrambi i protocolli soffrono di **overhead di tipo false sharing:**
Ipotizzando di avere due processori, se essi richiedessero due dati diversi, presenti però nella stessa cache line di memoria (nello stesso blocco), allora la modifica di un dato comporterebbe l'attivazione di uno tra i **protocolli invalidante o di update** per garantire la **cache coherency.**  
Questo comporta un degradimento delle performance! (cosa che nell'HPC è un problema.)  
Il fenomeno del false sharing avviene quando il dato su cui vogliono operare **non è lo stesso**, causando un rallentamento che in verità non è necessario!
Nota: Il fenomeno del false sharing **non avviene** quando tutti i processori che lavorano con quella cache line performano operazioni di **lettura** (se non si modificano i dati, ovviamente i protocolli invalidante o bloccante non vengono attivati: le caches non vengono invalidate o bloccate.)
#### Soluzioni al false sharing
Come risolvere il problema del false sharing? Utilizzando un **padding!**  
Ipotizziamo questa scena: abbiamo una cache line A di dimensione N (un array, avente diversi elementi):  
![](http://127.0.0.1:63161/paste-226e8bdde255e618f2265d665f4026d59bb73955.jpg)  
  
In questo caso, se due thread girassero su processori diversi e utilizzassero dati diversi (es: thr_1: 3; thr_2: 5), la modifica di uno dei due dati **genererebbe un fenomeno di false sharing.**  
  
Posso risolvere andando a definire **un array per ogni dato** della cache line, in modo tale da ottenere una **matrice:** la dimensione di ogni array sarà data da, appunto un **padding**, cioè dalla dimensione di ogni singola cache line di quel processore.  
  
In questo modo, il dato effettivo si troverà sempre in A[0], il resto è ridimensionamento. In questo modo, la modifica del dato non genererà **false sharing!** ![](http://127.0.0.1:63161/paste-4d200179b4c2830f33f2d6d3cee0dce45cd0930f.jpg)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMjE2ODk1MV19
-->